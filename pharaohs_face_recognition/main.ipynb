{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdd2d4c",
   "metadata": {},
   "source": [
    "# liberaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8974acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac345813",
   "metadata": {},
   "source": [
    "# read all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e08a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahmose I', 'Ahmose II', 'Ahmose-Meritamun', 'Ahmose-Nefertari', 'alexander theGreat', 'Amasis II', 'Amenemhat I', 'Amenemhat II', 'Amenemhat III', 'Amenemhat IV', 'Amenemhat V', 'Amenemope', 'Amenhotep I', 'Amenhotep II', 'Amenhotep III', 'Amenmesse', 'Ankhesenpepi II', 'Antiochus IV', 'Apries', 'Arsinoe III', 'Ay', 'cleopatara V', 'Cleopatra VII', 'Djedefre', 'Djoser', 'Hakor', 'Hatshepsut', 'Hetepheres II', 'Hor Awibre', 'Horemheb', 'Huni', 'Ikhnaton', 'Intef VI', 'Intef VII', 'Intef VIII', 'jcaesar', 'Kamose', 'Khafra', 'Khamerernebty II', 'Khasekhemwy', 'Khedebneithirbinet I', 'Khenemetneferhedjet I', 'khufo', 'Kiya', 'Marcus Antonius', 'Menkaura', 'Mentuhotep II', 'Mentuhotep III', 'Mentuhotep VI', 'Meresankh III', 'Meritamen', 'Meritaten', 'Merneptah', 'narmer', 'Necho II', 'Nectanebo I', 'Nectanebo II', 'Neferefra', 'Neferhotep I', 'Neferhotep_II', 'Neferneferuaten', 'Nefertari', 'nefertiti', 'Nofret II', 'nynetjer', 'Osorkon I', 'Osorkon II', 'pepi I', 'pepi II', 'Psamtik I', 'Psamtik II', 'Psamtik III', 'Psusennes I', 'Ptolemy II Philadelphos', 'Ptolemy III Euergetes', 'Ptolemy IV Philopator', 'Ptolemy V Epiphanes', 'Ptolemy_I', 'Ptolemy_XII', 'Ptolemy_X_Alexander_I', 'Ramesses I', 'Ramesses II', 'Ramesses III', 'Ramesses VII', 'Ramesses_IV', 'Ramses VI', 'Sahure', 'Senusret I', 'Senusret II', 'Senusret III', 'Seti I', 'Seti II', 'Shepetko', 'Shepseskaf', 'Sheshonq I', 'Shoshenq II', 'Siptah', 'Sitsnefru', 'smenkhkare', 'Snefru', 'Sobekhotep IV', 'Sobekhotep V', 'Sobekneferu', 'Taharqa', 'Teti', 'Tetisheri', 'Thutmose I', 'Thutmose III', 'Thutmose IV', 'Tiye', 'Tutankhamun', 'Twosret', 'Userkaf']\n"
     ]
    }
   ],
   "source": [
    "path='.//pharaohs_dataset'\n",
    "myList = os.listdir(path)\n",
    "known_imgs=[]\n",
    "names=[]\n",
    "for item in myList:\n",
    "    img=cv2.imread(f'{path}/{item}')\n",
    "    known_imgs.append(img)\n",
    "    names.append(os.path.splitext(item)[0])\n",
    "    \n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb9541",
   "metadata": {},
   "source": [
    "# Face_recognition\n",
    "Python library face_recognition. It works in a few steps:\n",
    "\n",
    "1. Identify a face in a given image\n",
    "2. Identify specific features in the face\n",
    "3. Generate a face encoding vector of 128 values\n",
    "\n",
    "Based on this encoding, we can measure the similarity between two face images â€” that can tell us if they belong to the same person.\n",
    "\n",
    "Let us check out an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feef5d",
   "metadata": {},
   "source": [
    "# encoding faces in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45aa408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16741762  0.01627867  0.08674538 -0.12592463 -0.11384147 -0.06947369\n",
      "  0.03461695 -0.05800762  0.17543454 -0.14334512  0.22286053 -0.08893844\n",
      " -0.21542694 -0.05143797 -0.01642086  0.21347345 -0.20953505 -0.14714341\n",
      " -0.02177672 -0.11744617  0.00250358  0.02334228 -0.02591448  0.08331191\n",
      " -0.07875627 -0.34232634 -0.1084146  -0.16341515  0.00500314 -0.07938371\n",
      "  0.0698802   0.10774039 -0.18520078 -0.05196153 -0.01942009  0.01262413\n",
      "  0.02551427 -0.08228411  0.13937163  0.00116432 -0.19376571 -0.08067117\n",
      "  0.07924522  0.25093433  0.11251695 -0.01771135  0.01678123 -0.06741301\n",
      "  0.06192773 -0.16236401 -0.03279245  0.07703635  0.06524973  0.0229878\n",
      " -0.04615821 -0.10171634  0.03905233  0.12383863 -0.23684327  0.04110302\n",
      "  0.09056261 -0.13121724 -0.08500606 -0.02953311  0.2967802   0.10925207\n",
      " -0.10618294 -0.16812378  0.26360139 -0.12429994 -0.07128998  0.02876223\n",
      " -0.16279472 -0.11401363 -0.25133717  0.03238161  0.41724437  0.07054862\n",
      " -0.10397873  0.07115801 -0.18640368  0.02217232  0.01520814  0.13933246\n",
      " -0.04277563 -0.01765418 -0.04250454  0.00933925  0.15845448 -0.01410412\n",
      " -0.02579327  0.16083659 -0.00377188 -0.00995602  0.02430425 -0.03584686\n",
      " -0.08251379 -0.01739334 -0.12613773 -0.06715795 -0.04010741 -0.0593389\n",
      " -0.00486195  0.13977943 -0.18316664  0.13513333  0.0316676  -0.04868543\n",
      "  0.00355304  0.10239287 -0.02158087 -0.11804152  0.11637169 -0.18598273\n",
      "  0.14303258  0.18527456  0.09764121  0.17146336  0.05033937  0.12039472\n",
      " -0.03990602 -0.04143589 -0.1883781  -0.00169674  0.10949157 -0.01523793\n",
      "  0.05646906  0.03168547]\n"
     ]
    }
   ],
   "source": [
    "encoded_known_imgs=[]\n",
    "for img in known_imgs:\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    encoded_img=face_recognition.face_encodings(img)[0]\n",
    "    encoded_known_imgs.append(encoded_img)\n",
    "    \n",
    "print(encoded_known_imgs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de4108da",
   "metadata": {},
   "source": [
    "# savinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dffbb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./saved_point.pkl\", \"wb\")\n",
    "pickle.dump({\"Names\":names, \"Encodes\": encoded_known_imgs}, file)\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27987dea",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b8fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./saved_point.pkl\", \"rb\")\n",
    "saved_point = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "names, encoded_known_imgs = saved_point[\"Names\"], saved_point[\"Encodes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e18552",
   "metadata": {},
   "source": [
    "## video cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a543c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    success,frame_ =cap.read()\n",
    "    frame  = cv2.resize(frame_ , (0, 0), None, 0.25, 0.25)\n",
    "    frame =cv2.cvtColor(frame ,cv2.COLOR_BGR2RGB)\n",
    "    faces_location =face_recognition.face_locations(frame)\n",
    "    feces_encoded =face_recognition.face_encodings(frame,faces_location)\n",
    "    \n",
    "    for encodedFace, face_Loc in zip (feces_encoded ,faces_location ):\n",
    "        matches=face_recognition.compare_faces(encoded_known_imgs,encodedFace)\n",
    "        face_distances =face_recognition.face_distance(encoded_known_imgs,encodedFace)\n",
    "        best_match=np.argmin(face_distances )\n",
    "        # print(best_match)\n",
    "    \n",
    "        if matches[best_match]:\n",
    "            name=names[best_match]\n",
    "            # print(name)\n",
    "            y1, x2, y2, x1 = face_Loc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "            cv2.rectangle(frame_, (x1, y1), (x2+15, y2), (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame_, (x1, y2-30 ), (x2+15, y2), (0, 0, 255), cv2.FILLED)  \n",
    "            cv2.putText(frame_, name, (x1 + 5, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 1)\n",
    "    \n",
    "       \n",
    "         \n",
    "    cv2.imshow('video', frame_)\n",
    "    k = cv2.waitKey(1)\n",
    "    if ord('q') == k:       # press Q key on keyboard to quite\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134000d",
   "metadata": {},
   "source": [
    "## test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c9dac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m best_match\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39margmin(face_distances )\n\u001b[1;32m      9\u001b[0m \u001b[39m#print(best_match)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mif\u001b[39;00m matches[best_match]:\n\u001b[1;32m     12\u001b[0m     name\u001b[39m=\u001b[39mnames[best_match]\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matches' is not defined"
     ]
    }
   ],
   "source": [
    "test=cv2.imread('./test_data/Ahmose I.jpg')\n",
    "test =cv2.cvtColor(test ,cv2.COLOR_BGR2RGB)\n",
    "faces_location =face_recognition.face_locations(test)\n",
    "feces_encoded =face_recognition.face_encodings(test,faces_location)\n",
    "for encodedFace, face_Loc in zip (feces_encoded ,faces_location ):\n",
    "    matches=face_recognition.compare_faces(encoded_known_imgs,encodedFace)\n",
    "    face_distances =face_recognition.face_distance(encoded_known_imgs,encodedFace)\n",
    "    best_match=np.argmin(face_distances )\n",
    "    #print(best_match)\n",
    "\n",
    "    if matches[best_match]:\n",
    "        name=names[best_match]\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
